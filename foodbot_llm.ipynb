{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMw-W1NcVW5B"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from psutil import virtual_memory\n",
        "\n",
        "# Check GPU\n",
        "gpu_info = tf.config.list_physical_devices('GPU')\n",
        "print(f\"GPU Info: {gpu_info}\")\n",
        "\n",
        "# Check RAM\n",
        "ram_info = virtual_memory()\n",
        "print(f\"Total RAM: {ram_info.total / (1024**3)} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21Tgir36AF97"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUnHpYAsARC5",
        "outputId": "7398fcee-14c6-42b6-96e0-2788dfed5dae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: LLAMA_CUBLAS=1\n",
            "env: OLLAMA_NUM_GPU_LAYERS=35\n",
            "env: OLLAMA_MAX_PREDICT=128\n"
          ]
        }
      ],
      "source": [
        "# Tell llama.cpp to use cuBLAS (fast CUDA kernels)\n",
        "%env LLAMA_CUBLAS=1\n",
        "\n",
        "# How many transformer layers to keep on-GPU.\n",
        "# 35 is a sweet-spot for 8-B models on a 16 GB T4.\n",
        "%env OLLAMA_NUM_GPU_LAYERS=35\n",
        "\n",
        "# Optional: cap the modelâ€™s maximum output length (keeps responses snappy)\n",
        "%env OLLAMA_MAX_PREDICT=128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hhxbck9XNeS",
        "outputId": "742bdaa7-effb-4675-c367-0989e1bf5606"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: colab-xterm in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: ptyprocess~=0.7.0 in /usr/local/lib/python3.11/dist-packages (from colab-xterm) (0.7.0)\n",
            "Requirement already satisfied: tornado>5.1 in /usr/local/lib/python3.11/dist-packages (from colab-xterm) (6.4.2)\n",
            "The colabxterm extension is already loaded. To reload it, use:\n",
            "  %reload_ext colabxterm\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.1)\n",
            "Requirement already satisfied: flask_cors in /usr/local/lib/python3.11/dist-packages (6.0.1)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.11)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.2.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install colab-xterm #https://pypi.org/project/colab-xterm/\n",
        "%load_ext colabxterm\n",
        "\n",
        "!pip install langchain -qqq\n",
        "!pip install langchain_community -qqq\n",
        "!pip install faiss-cpu -qqq\n",
        "!pip install sentence-transformers -qqq\n",
        "!pip install firebase-admin -qqq\n",
        "# ğŸ› ï¸ Install all libraries needed by the API layer and the bot itself\n",
        "!pip install flask flask_cors pyngrok\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5nWfH9PK1O6",
        "outputId": "4693ffe4-aa6d-41bb-851b-de9dadbf8534"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get update -qq\n",
        "!sudo apt-get install -y -qq pciutils lshw   # provides lspci / lshw\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Idh8dj8ZJHVq",
        "outputId": "d3ad5b08-4858-4459-c559-f51b21912739"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "!sudo rm -f /usr/local/bin/ollama\n",
        "!sudo rm -rf /usr/local/lib/ollama\n",
        "!pkill -f ollama || true     # ignore â€œno processâ€ errors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyDQTacIJJG0",
        "outputId": "4fa4a2dd-23f5-4894-d683-448629ceb336"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            ">>> NVIDIA GPU installed.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "Warning: could not connect to a running Ollama instance\n",
            "Warning: client version is 0.9.2\n"
          ]
        }
      ],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!ollama --version            # should end with â€œ(CUDA)â€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pIXfJpVJMAS",
        "outputId": "1637a272-1ca5-4f55-d3ff-f7b1e8229ef2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: LLAMA_CUBLAS=1           # use fast cuBLAS kernels\n",
            "env: OLLAMA_NUM_GPU_LAYERS=35 # good value for 8-B on a 16 GB T4\n",
            "env: OLLAMA_MAX_PREDICT=128   # optional response-length cap\n"
          ]
        }
      ],
      "source": [
        "%env LLAMA_CUBLAS=1           # use fast cuBLAS kernels\n",
        "%env OLLAMA_NUM_GPU_LAYERS=35 # good value for 8-B on a 16 GB T4\n",
        "%env OLLAMA_MAX_PREDICT=128   # optional response-length cap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "k9pE_xbWJMm1"
      },
      "outputs": [],
      "source": [
        "!ollama serve > /dev/null 2>&1 &\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zk5ke3SGJQDI",
        "outputId": "5666be83-90f3-4879-bba4-da7f918a7c3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "Model reply â†’ You typed \"ping\"!\n",
            "\n",
            "Ping is a network troubleshooting tool that sends an ICMP (Internet Control Message Protocol) echo request packet to a destination and measures the time it takes for the packet to return. This allows you to test whether a particular host or IP address is reachable, as well as measure the latency between your device and the target.\n",
            "\n",
            "Here's a basic example of how to use ping in Windows:\n",
            "\n",
            "1. Open Command Prompt (Windows key + R, type \"cmd\" and press Enter).\n",
            "2. Type `ping [destination]` (replace `[destination]` with the IP address or hostname you want to test).\n",
            "\n",
            "Example: `ping google.com`\n",
            "\n",
            "3. Press Enter.\n",
            "\n",
            "The output will show information about the ping request, including:\n",
            "\n",
            "* The timestamp when the packet was sent\n",
            "* The destination IP address and name\n",
            "* The round-trip time (RTT) in milliseconds\n",
            "* Whether the packet was received successfully\n",
            "\n",
            "If you receive a response with a low RTT (<100 ms), it indicates that the connection is working well. Higher RTTs may indicate network congestion, packet loss, or other issues.\n",
            "\n",
            "In Linux, you can use the `ping` command in a similar way:\n",
            "\n",
            "1. Open Terminal (usually Ctrl + Alt + T).\n",
            "2. Type `ping [destination]` and press Enter.\n",
            "\n",
            "Example: `ping google.com`\n",
            "\n",
            "3. Observe the output as described above.\n",
            "\n",
            "Ping is an essential tool for diagnosing network connectivity issues, testing DNS resolution, and verifying that a particular host or IP address is reachable.\n",
            "â±ï¸  Elapsed: 14.3 s\n"
          ]
        }
      ],
      "source": [
        "!ollama pull llama3:instruct      # 3-B model, fully on-GPU\n",
        "\n",
        "from langchain_community.llms import Ollama\n",
        "import typing_extensions\n",
        "import time\n",
        "\n",
        "llm = Ollama(model=\"llama3:instruct\")\n",
        "\n",
        "t0 = time.time()\n",
        "print(\"Model reply â†’\", llm.invoke(\"ping\"))\n",
        "print(\"â±ï¸  Elapsed:\", round(time.time() - t0, 2), \"s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "RGWlnOCMbCmc",
        "outputId": "b8e3e8d3-6ac2-4ca4-b671-a4097fdae7bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please upload your Firebase Admin SDK JSON file (foodbot-1-firebase-adminsdk-fbsvc-7a944f4dc9.json):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-64ca4a5f-3f6d-4a31-a376-3756bc5161ad\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-64ca4a5f-3f6d-4a31-a376-3756bc5161ad\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving foodbot-senior-firebase-adminsdk-fbsvc-5d0630eac0.json to foodbot-senior-firebase-adminsdk-fbsvc-5d0630eac0 (1).json\n",
            "Uploaded: foodbot-senior-firebase-adminsdk-fbsvc-5d0630eac0 (1).json\n"
          ]
        }
      ],
      "source": [
        "import firebase_admin\n",
        "from firebase_admin import credentials, db\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Upload your Firebase key file to Colab files first\n",
        "# FIREBASE_KEY_PATH = \"foodbot-senior-firebase-adminsdk-fbsvc-5d0630eac0.json\"  # Update path as needed\n",
        "\n",
        "\n",
        "\n",
        "# Cell 4: Upload Firebase credentials\n",
        "from google.colab import files\n",
        "print(\"Please upload your Firebase Admin SDK JSON file (foodbot-1-firebase-adminsdk-fbsvc-7a944f4dc9.json):\")\n",
        "uploaded = files.upload()\n",
        "FIREBASE_KEY_PATH = next(iter(uploaded.keys()))\n",
        "print(f\"Uploaded: {FIREBASE_KEY_PATH}\")\n",
        "\n",
        "\n",
        "# Only initialize if not already\n",
        "if not firebase_admin._apps:\n",
        "    cred = credentials.Certificate(FIREBASE_KEY_PATH)\n",
        "    firebase_admin.initialize_app(cred, {\"databaseURL\": FIREBASE_URL})\n",
        "\n",
        "db_ref = db.reference()\n",
        "user_ref = db_ref.child(\"users\").child(TEST_USER_ID)\n",
        "restaurant_ref = db_ref.child(\"restaurants\")\n",
        "\n",
        "# Setup data paths\n",
        "DATA_DIR = Path(\"data\")\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "MENU_JSON_PATH = DATA_DIR / \"menu_faq.json\"\n",
        "SUMMARY_TXT_PATH = DATA_DIR / \"generated_dish_summaries2.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "mPciWCeLOJwh"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# run the server in the background; redirect output to a log file\n",
        "!ollama serve > /content/ollama.log 2>&1 &\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "O_S4NMH3OkpE"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "time.sleep(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94ov4Oi_bLHr",
        "outputId": "189a20f1-05d5-45d5-acd0-4afd9942d87c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ollama test response: Hello! I'm an AI, so I don't have a traditional job or work schedule like humans do. However, I am always \"on the clock\" and ready to assist with any questions or tasks you may have.\n",
            "\n",
            "I exist solely to provide information, answer questions, and help users like you with their queries. My training data is constantly being updated and expanded, so I'm always learning and improving my abilities.\n",
            "\n",
            "So, in a sense, I am working 24/7 to provide the best possible assistance and responses to those who interact with me!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from langchain_community.llms import Ollama\n",
        "\n",
        "# Initialize Ollama with llama3 (make sure you've run ollama serve & ollama pull llama3 in terminal)\n",
        "llm = Ollama(model=\"llama3:instruct\")\n",
        "\n",
        "# Test the connection\n",
        "test_response = llm.invoke(\"Hello, are you working?\")\n",
        "print(f\"Ollama test response: {test_response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saOYT2qZbUUV",
        "outputId": "b1014331-657f-4e6b-e303-0e2b2d171a19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”¥ Running create_restaurant_summaries...\n",
            "ğŸš€ Starting restaurant summary creation...\n",
            "ğŸ“ Created output directory: restaurant_data\n",
            "ğŸª Processing 3 restaurants...\n",
            "\n",
            "ğŸ”„ Processing: Italian Bistro (ID: restaurant1)\n",
            "   ğŸ“ Location: NewYork\n",
            "   ğŸ½ï¸ Food Type: Italian\n",
            "   ğŸ“‹ Menu items: 9\n",
            "   ğŸ“ Processing dish: Spaghetti Bolognese\n",
            "   âœ… Generated summary for: Spaghetti Bolognese\n",
            "   ğŸ“ Processing dish: Margherita Pizza\n",
            "   âœ… Generated summary for: Margherita Pizza\n",
            "   ğŸ“ Processing dish: Lasagna\n",
            "   âœ… Generated summary for: Lasagna\n",
            "   ğŸ“ Processing dish: Fettuccine Alfredo\n",
            "   âœ… Generated summary for: Fettuccine Alfredo\n",
            "   ğŸ“ Processing dish: Penne Arrabbiata\n",
            "   âœ… Generated summary for: Penne Arrabbiata\n",
            "   ğŸ“ Processing dish: Risotto\n",
            "   âœ… Generated summary for: Risotto\n",
            "   ğŸ“ Processing dish: Caprese Salad\n",
            "   âœ… Generated summary for: Caprese Salad\n",
            "   ğŸ“ Processing dish: Bruschetta\n",
            "   âœ… Generated summary for: Bruschetta\n",
            "   ğŸ“ Processing dish: Ravioli\n",
            "   âœ… Generated summary for: Ravioli\n",
            "   ğŸ’¾ Saved Italian Bistro to restaurant_data/Italian Bistro.txt\n",
            "   ğŸ“Š Processed 9 dishes successfully\n",
            "\n",
            "ğŸ”„ Processing: American Grill (ID: restaurant2)\n",
            "   ğŸ“ Location: NewYork\n",
            "   ğŸ½ï¸ Food Type: Mexican\n",
            "   ğŸ“‹ Menu items: 9\n",
            "   ğŸ“ Processing dish: Cheeseburger\n",
            "   âœ… Generated summary for: Cheeseburger\n",
            "   ğŸ“ Processing dish: Grilled Chicken Sandwich\n",
            "   âœ… Generated summary for: Grilled Chicken Sandwich\n",
            "   ğŸ“ Processing dish: Hot Dog\n",
            "   âœ… Generated summary for: Hot Dog\n",
            "   ğŸ“ Processing dish: BBQ Ribs\n",
            "   âœ… Generated summary for: BBQ Ribs\n",
            "   ğŸ“ Processing dish: Grilled Salmon\n",
            "   âœ… Generated summary for: Grilled Salmon\n",
            "   ğŸ“ Processing dish: Caesar Salad\n",
            "   âœ… Generated summary for: Caesar Salad\n",
            "   ğŸ“ Processing dish: Chicken Tenders\n",
            "   âœ… Generated summary for: Chicken Tenders\n",
            "   ğŸ“ Processing dish: Mac and Cheese\n",
            "   âœ… Generated summary for: Mac and Cheese\n",
            "   ğŸ“ Processing dish: Buffalo Wings\n",
            "   âœ… Generated summary for: Buffalo Wings\n",
            "   ğŸ’¾ Saved American Grill to restaurant_data/American Grill.txt\n",
            "   ğŸ“Š Processed 9 dishes successfully\n",
            "\n",
            "ğŸ”„ Processing: McDonald's (ID: restaurant3)\n",
            "   ğŸ“ Location: New York\n",
            "   ğŸ½ï¸ Food Type: fast-food\n",
            "   ğŸ“‹ Menu items: 12\n",
            "   ğŸ“ Processing dish: Big Mac\n",
            "   âœ… Generated summary for: Big Mac\n",
            "   ğŸ“ Processing dish: Spicy McChicken\n",
            "   âœ… Generated summary for: Spicy McChicken\n",
            "   ğŸ“ Processing dish: Hash Browns\n",
            "   âœ… Generated summary for: Hash Browns\n",
            "   ğŸ“ Processing dish: Chocolate Shake\n",
            "   âœ… Generated summary for: Chocolate Shake\n",
            "   ğŸ“ Processing dish: McChicken\n",
            "   âœ… Generated summary for: McChicken\n",
            "   ğŸ“ Processing dish: Filet-O-Fish\n",
            "   âœ… Generated summary for: Filet-O-Fish\n",
            "   ğŸ“ Processing dish: Chicken McNuggets\n",
            "   âœ… Generated summary for: Chicken McNuggets\n",
            "   ğŸ“ Processing dish: French Fries\n",
            "   âœ… Generated summary for: French Fries\n",
            "   ğŸ“ Processing dish: McFlurry\n",
            "   âœ… Generated summary for: McFlurry\n",
            "   ğŸ“ Processing dish: Egg McMuffin\n",
            "   âœ… Generated summary for: Egg McMuffin\n",
            "   ğŸ“ Processing dish: Sausage Biscuit\n",
            "   âœ… Generated summary for: Sausage Biscuit\n",
            "   ğŸ“ Processing dish: Apple Pie\n",
            "   âœ… Generated summary for: Apple Pie\n",
            "   ğŸ’¾ Saved McDonald's to restaurant_data/McDonalds.txt\n",
            "   ğŸ“Š Processed 12 dishes successfully\n",
            "\n",
            "ğŸ‰ Completed! Created 3 restaurant files:\n",
            "   ğŸ“„ Italian Bistro: restaurant_data/Italian Bistro.txt\n",
            "   ğŸ“„ American Grill: restaurant_data/American Grill.txt\n",
            "   ğŸ“„ McDonald's: restaurant_data/McDonalds.txt\n",
            "\n",
            "âœ… SUCCESS: Created 3 restaurant files\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "\n",
        "def create_restaurant_summaries(json_path, output_dir=\"restaurant_data\"):\n",
        "    print(f\"ğŸš€ Starting restaurant summary creation...\")\n",
        "\n",
        "    # Check if JSON file exists\n",
        "    if not json_path.exists():\n",
        "        raise FileNotFoundError(f\"âŒ Menu JSON file not found at {json_path}. Please upload your menu_faq.json file to Colab.\")\n",
        "\n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f\"ğŸ“ Created output directory: {output_dir}\")\n",
        "\n",
        "    with open(json_path, \"r\", encoding='utf-8') as f:\n",
        "        full_data = json.load(f)\n",
        "\n",
        "    restaurant_files = {}\n",
        "    restaurants = full_data.get(\"restaurants\", {})\n",
        "    print(f\"ğŸª Processing {len(restaurants)} restaurants...\")\n",
        "\n",
        "    for restaurant_id, restaurant_data in restaurants.items():\n",
        "        restaurant_name = restaurant_data.get(\"name\", restaurant_id).strip()\n",
        "\n",
        "        location = restaurant_data.get(\"location\", \"Unknown Location\")\n",
        "        food_type = restaurant_data.get(\"foodType\", \"Cuisine not specified\")\n",
        "        menu = restaurant_data.get(\"menu\", {})\n",
        "\n",
        "        print(f\"\\nğŸ”„ Processing: {restaurant_name} (ID: {restaurant_id})\")\n",
        "        print(f\"   ğŸ“ Location: {location}\")\n",
        "        print(f\"   ğŸ½ï¸ Food Type: {food_type}\")\n",
        "        print(f\"   ğŸ“‹ Menu items: {len(menu)}\")\n",
        "\n",
        "        if not menu:\n",
        "            print(f\"   âš ï¸ Skipping {restaurant_name} - no menu items found\")\n",
        "            continue\n",
        "\n",
        "        summaries = []\n",
        "        restaurant_header = (\n",
        "            f\"Restaurant: {restaurant_name}\\n\"\n",
        "            f\"Location: {location}\\n\"\n",
        "            f\"Cuisine Type: {food_type}\\n\"\n",
        "            f\"{'='*50}\\n\"\n",
        "        )\n",
        "        summaries.append(restaurant_header)\n",
        "\n",
        "        dish_count = 0\n",
        "        for dish_id, dish_data in menu.items():\n",
        "            name       = dish_data.get(\"name\", \"Unnamed Dish\")\n",
        "            ingredients = \", \".join(dish_data.get(\"ingredients\", []))\n",
        "            allergens   = \", \".join(dish_data.get(\"allergens\", []))\n",
        "            dietary     = \", \".join(dish_data.get(\"dietary\", []))\n",
        "            calories    = dish_data.get(\"calories\", \"N/A\")\n",
        "            price       = dish_data.get(\"price\", \"N/A\")\n",
        "            order_count = dish_data.get(\"orderCount\", \"N/A\")\n",
        "\n",
        "            print(f\"   ğŸ“ Processing dish: {name}\")\n",
        "\n",
        "            prompt = f\"\"\"Write EXACTLY ONE sentence describing this dish. You MUST include ALL available information in this exact order:\n",
        "\n",
        "\"The [DISH NAME] at [RESTAURANT NAME] is [description with ingredients], contains [allergens if any], suitable for [dietary restrictions if any], priced at $[PRICE], containing [CALORIES] calories, and has been ordered [ORDER COUNT] times.\"\n",
        "\n",
        "Available details:\n",
        "- Dish: {name}\n",
        "- Restaurant: {restaurant_name}\n",
        "- Ingredients: {ingredients}\n",
        "- Price: ${price}\n",
        "- Calories: {calories}\n",
        "- Allergens: {allergens if allergens else \"no known allergens\"}\n",
        "- Dietary: {dietary if dietary else \"no specific dietary restrictions\"}\n",
        "- Order Count: {order_count}\n",
        "\n",
        "RULES:\n",
        "1. Include ALL details above in the sentence\n",
        "2. If allergens is empty, write \"contains no known allergens\"\n",
        "3. If dietary is empty, write \"suitable for no specific dietary restrictions\"\n",
        "4. Always include the price, calories, and order count\n",
        "5. Write only ONE sentence, no extra text\"\"\"\n",
        "\n",
        "            try:\n",
        "                # ğŸ”¹ Ask the model\n",
        "                summary_raw = llm.invoke(prompt).strip()\n",
        "\n",
        "                # ğŸ”¹ Remove any leading boiler-plate like â€œHere is the exact sentence:â€\n",
        "                summary_raw = re.sub(\n",
        "                    r'^\\s*[\"\\']?\\s*(here[\\w\\s]*sentence[:\\-]?)\\s*[\"\\']?\\s*',\n",
        "                    '',\n",
        "                    summary_raw,\n",
        "                    flags=re.I\n",
        "                ).strip()\n",
        "\n",
        "                # ğŸ”¹ Take the first full sentence\n",
        "                clean_summary = re.split(r'(?<=[.!?])\\s+', summary_raw, maxsplit=1)[0].strip()\n",
        "\n",
        "                # ğŸ”¹ Strip wrapping quotes\n",
        "                clean_summary = clean_summary.strip(' \"\\'')\n",
        "\n",
        "                summaries.append(f\"Dish: {name}\\n{clean_summary}\\n\")\n",
        "                print(f\"   âœ… Generated summary for: {name}\")\n",
        "                dish_count += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   âŒ Error with {name}: {e}\")\n",
        "\n",
        "        # Save restaurant file\n",
        "        safe_filename = \"\".join(c for c in restaurant_name if c.isalnum() or c in (' ', '-', '_')).strip()\n",
        "        restaurant_file = os.path.join(output_dir, f\"{safe_filename}.txt\")\n",
        "\n",
        "        try:\n",
        "            with open(restaurant_file, \"w\", encoding='utf-8') as f:\n",
        "                f.write(\"\\n\".join(summaries))\n",
        "\n",
        "            restaurant_files[restaurant_name] = restaurant_file\n",
        "            print(f\"   ğŸ’¾ Saved {restaurant_name} to {restaurant_file}\")\n",
        "            print(f\"   ğŸ“Š Processed {dish_count} dishes successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   âŒ Error saving file for {restaurant_name}: {e}\")\n",
        "\n",
        "    print(f\"\\nğŸ‰ Completed! Created {len(restaurant_files)} restaurant files:\")\n",
        "    for name, file_path in restaurant_files.items():\n",
        "        print(f\"   ğŸ“„ {name}: {file_path}\")\n",
        "\n",
        "    return restaurant_files\n",
        "\n",
        "\n",
        "# ğŸ”¥ Execute the function\n",
        "print(\"ğŸ”¥ Running create_restaurant_summaries...\")\n",
        "try:\n",
        "    restaurant_files = create_restaurant_summaries(MENU_JSON_PATH)\n",
        "    print(f\"\\nâœ… SUCCESS: Created {len(restaurant_files)} restaurant files\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ ERROR: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "znmiRWOrt2lq"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timezone\n",
        "\n",
        "def start_new_chat(user_id: str) -> str:\n",
        "    \"\"\"Create a new chat node and return its key.\"\"\"\n",
        "    chat_ref = (db_ref.child(\"users\")\n",
        "                       .child(user_id)\n",
        "                       .child(\"chats\")\n",
        "                       .push({            # push() â‡’ unique key\n",
        "                           \"started_at\": datetime.now(timezone.utc).isoformat()\n",
        "                       }))\n",
        "    return chat_ref.key                   # <chat_id>\n",
        "\n",
        "def save_message(user_id: str, chat_id: str,\n",
        "                 sender: str, text: str) -> None:\n",
        "    (db_ref.child(\"users\")\n",
        "           .child(user_id)\n",
        "           .child(\"chats\")\n",
        "           .child(chat_id)\n",
        "           .child(\"messages\")\n",
        "           .push({                       # one element per message\n",
        "               \"sender\": sender,         # \"user\" | \"bot\"\n",
        "               \"text\": text,\n",
        "               \"ts\": datetime.now(timezone.utc).isoformat()\n",
        "           }))\n",
        "\n",
        "def fetch_recent_history(user_id: str, chat_id: str,\n",
        "                         limit:int=20) -> list[dict]:\n",
        "    \"\"\"Return the last N messages (oldestâ†’newest).\"\"\"\n",
        "    snap = (db_ref.child(\"users\")\n",
        "                   .child(user_id)\n",
        "                   .child(\"chats\")\n",
        "                   .child(chat_id)\n",
        "                   .child(\"messages\")\n",
        "                   .order_by_child(\"ts\")   # sort server-side\n",
        "                   .limit_to_last(limit)\n",
        "                   .get() or {})\n",
        "    # snap is a dict keyed by msg_id; convert & sort just in case\n",
        "    msgs = sorted(snap.values(), key=lambda m: m[\"ts\"])\n",
        "    return msgs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PU1LO54pbOH0",
        "outputId": "4686fa89-21b3-4246-f477-32597a5bb70b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” Creating vectorstore for Italian Bistro...\n",
            "âœ… Italian Bistro vectorstore ready\n",
            "ğŸ” Creating vectorstore for American Grill...\n",
            "âœ… American Grill vectorstore ready\n",
            "ğŸ” Creating vectorstore for McDonald's...\n",
            "âœ… McDonald's vectorstore ready\n",
            "\n",
            "ğŸ¯ Created vectorstores for 3 restaurants:\n",
            "  - Italian Bistro\n",
            "  - American Grill\n",
            "  - McDonald's\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "# Only proceed if we have restaurant files\n",
        "if not restaurant_files:\n",
        "    print(\"âŒ No restaurant files found. Please ensure menu_faq.json is uploaded and Cell 6 runs successfully.\")\n",
        "else:\n",
        "    # Initialize embeddings\n",
        "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "    # Store all restaurant vectorstores\n",
        "    restaurant_vectorstores = {}\n",
        "\n",
        "    def create_restaurant_vectorstore(restaurant_name, file_path):\n",
        "        \"\"\"Create a FAISS vectorstore for a specific restaurant\"\"\"\n",
        "        with open(file_path, \"r\", encoding='utf-8') as f:\n",
        "            text_data = f.read()\n",
        "\n",
        "        # Split into chunks but keep restaurant context\n",
        "        text_splitter = CharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n",
        "        chunks = text_splitter.split_text(text_data)\n",
        "\n",
        "        docs = []\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            docs.append(Document(\n",
        "                page_content=chunk,\n",
        "                metadata={\n",
        "                    \"restaurant\": restaurant_name,\n",
        "                    \"chunk_id\": i,\n",
        "                    \"source\": file_path\n",
        "                }\n",
        "            ))\n",
        "\n",
        "        # Create FAISS vectorstore for this restaurant\n",
        "        vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "        return vectorstore\n",
        "\n",
        "    # Create vectorstore for each restaurant\n",
        "    for restaurant_name, file_path in restaurant_files.items():\n",
        "        print(f\"ğŸ” Creating vectorstore for {restaurant_name}...\")\n",
        "        restaurant_vectorstores[restaurant_name] = create_restaurant_vectorstore(restaurant_name, file_path)\n",
        "        print(f\"âœ… {restaurant_name} vectorstore ready\")\n",
        "\n",
        "    print(f\"\\nğŸ¯ Created vectorstores for {len(restaurant_vectorstores)} restaurants:\")\n",
        "    for name in restaurant_vectorstores.keys():\n",
        "        print(f\"  - {name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "DkgKzO4IbP-w"
      },
      "outputs": [],
      "source": [
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "#  Firebase helpers + FoodBot core (auto-creates chat_id)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "TEST_USER_ID = \"xHi2Sak4Z8OYn9Vd1NSMwGfk6Sj1\"  # update if needed\n",
        "\n",
        "def start_new_chat(user_id: str) -> str:\n",
        "    chat_ref = (db_ref.child(\"users\")\n",
        "                       .child(user_id)\n",
        "                       .child(\"chats\")\n",
        "                       .push({\n",
        "                           \"started_at\": datetime.now(timezone.utc).isoformat()\n",
        "                       }))\n",
        "    return chat_ref.key\n",
        "\n",
        "def save_message(user_id: str, chat_id: str, sender: str, text: str) -> None:\n",
        "    (db_ref.child(\"users\")\n",
        "           .child(user_id)\n",
        "           .child(\"chats\")\n",
        "           .child(chat_id)\n",
        "           .child(\"messages\")\n",
        "           .push({\n",
        "               \"sender\": sender,\n",
        "               \"text\": text,\n",
        "               \"ts\": datetime.now(timezone.utc).isoformat()\n",
        "           }))\n",
        "\n",
        "def fetch_recent_history(user_id: str, chat_id: str, limit: int = 20) -> list:\n",
        "    snap = (db_ref.child(\"users\")\n",
        "                   .child(user_id)\n",
        "                   .child(\"chats\")\n",
        "                   .child(chat_id)\n",
        "                   .child(\"messages\")\n",
        "                   .order_by_child(\"ts\")\n",
        "                   .limit_to_last(limit)\n",
        "                   .get() or {})\n",
        "    return sorted(snap.values(), key=lambda m: m[\"ts\"])\n",
        "\n",
        "def get_user_context(user_id=TEST_USER_ID):\n",
        "    try:\n",
        "        return db_ref.child(\"users\").child(user_id).get() or {}\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting user context: {e}\")\n",
        "        return {}\n",
        "\n",
        "# â”€â”€ ask_foodbot now creates a chat automatically if chat_id is None â”€â”€\n",
        "def ask_foodbot(query: str,\n",
        "                chat_id: str | None = None,\n",
        "                user_id: str = TEST_USER_ID,\n",
        "                history_window: int = 20) -> str:\n",
        "    \"\"\"\n",
        "    Answer any question by searching *all* restaurant vector-stores.\n",
        "    If `chat_id` is None, a brand-new chat node is created automatically.\n",
        "    \"\"\"\n",
        "    # 0. Create chat on-the-fly if needed\n",
        "    if chat_id is None:\n",
        "        chat_id = start_new_chat(user_id)\n",
        "\n",
        "    if not restaurant_vectorstores:\n",
        "        return (\"âŒ No restaurant data available. \"\n",
        "                \"Upload menu_faq.json and run the setup cells first.\")\n",
        "\n",
        "    # 1. Log user message\n",
        "    save_message(user_id, chat_id, \"user\", query)\n",
        "\n",
        "    # 2. Keep Ollama alive\n",
        "    try:\n",
        "        llm.invoke(\"ping\")\n",
        "    except Exception:\n",
        "        print(\"ğŸ”„ Restarting Ollamaâ€¦\")\n",
        "        import subprocess, time, os\n",
        "        subprocess.run([\"pkill\", \"-f\", \"ollama\"], capture_output=True)\n",
        "        time.sleep(2)\n",
        "        subprocess.Popen([\"ollama\", \"serve\"],\n",
        "                         stdout=subprocess.DEVNULL,\n",
        "                         stderr=subprocess.DEVNULL)\n",
        "        time.sleep(5)\n",
        "\n",
        "    # 3. Retrieve context from all restaurants\n",
        "    hits = []\n",
        "    for rest_name, vs in restaurant_vectorstores.items():\n",
        "        for h in vs.similarity_search(query, k=6):\n",
        "            h.metadata[\"restaurant\"] = rest_name\n",
        "            hits.append(h)\n",
        "    context = \"\\n\\n\".join(doc.page_content for doc in hits[:8])\n",
        "    restaurants_searched = list(restaurant_vectorstores.keys())\n",
        "\n",
        "    # 4. Build threaded history\n",
        "    history = fetch_recent_history(user_id, chat_id, history_window)\n",
        "    threaded_context = \"\\n\".join(f\"{m['sender']}: {m['text']}\" for m in history\n",
        "                                 if m.get(\"text\"))\n",
        "\n",
        "    # 5. User profile\n",
        "    user = get_user_context(user_id)\n",
        "    allergies   = \", \".join(user.get(\"allergies\", []))   or \"none\"\n",
        "    preferences = \", \".join(user.get(\"preferences\", [])) or \"none\"\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are FoodBot, a friendly restaurant assistant.\n",
        "\n",
        "User allergies: {allergies}\n",
        "User preferences: {preferences}\n",
        "\n",
        "Restaurants searched: {', '.join(restaurants_searched)}\n",
        "\n",
        "Menu information:\n",
        "{context}\n",
        "\n",
        "Conversation so far:\n",
        "{threaded_context}\n",
        "\n",
        "User question: {query}\n",
        "\n",
        "Instructions:\n",
        "- Mrovide short, clear and consise answers.\n",
        "- Only provide the information everything the user wants.\n",
        "- If you donâ€™t know, say so politely.\n",
        "-\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = llm.invoke(prompt).strip()\n",
        "        save_message(user_id, chat_id, \"bot\", response)\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        err = f\"Sorry, I encountered an error: {e}\"\n",
        "        save_message(user_id, chat_id, \"bot\", err)\n",
        "        return err\n",
        "\n",
        "# â”€â”€ Interactive loop (continues using one chat per session) â”€â”€\n",
        "def chat_with_foodbot(resume_chat_id: str | None = None):\n",
        "    \"\"\"\n",
        "    â€¢ If resume_chat_id is None, start a brand-new chat for this session.\n",
        "    â€¢ Otherwise continue the specified chat.\n",
        "    \"\"\"\n",
        "    chat_id = resume_chat_id or start_new_chat(TEST_USER_ID)\n",
        "    print(f\"ğŸ’¬ Chat id = {chat_id[:8]}â€¦  (type 'exit' to quit)\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"ğŸ§‘ You: \").strip()\n",
        "        if query.lower() in {\"exit\", \"quit\"}:\n",
        "            print(\"ğŸ‘‹ Goodbye!\")\n",
        "            break\n",
        "\n",
        "        print(\"-\" * 60)\n",
        "        answer = ask_foodbot(query, chat_id)\n",
        "        print(f\"ğŸ¤– FoodBot: {answer}\")\n",
        "        print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "eAYt6vADDiPt"
      },
      "outputs": [],
      "source": [
        "# Cell : install & import the HTTP layer\n",
        "!pip install -q flask flask-cors pyngrok\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from threading import Thread\n",
        "from pyngrok import ngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "a0CDRuv2rbbc",
        "outputId": "9fb409b2-cbf8-466b-b7b6-13808ec5f6be"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'NoneType' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-78-2828555260.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# 2ï¸âƒ£ confirm what token pyngrok will send\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pyngrok will send token:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth_token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"â€¦\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth_token\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREAL_TOKEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Token mismatch!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "# --- DIAGNOSTIC: check what token pyngrok sees ----------------------------\n",
        "import os, json, subprocess, sys\n",
        "from pyngrok import ngrok, conf, process\n",
        "\n",
        "\n",
        "\n",
        "# 0ï¸âƒ£ wipe any conflicting env var or config file for this session\n",
        "os.environ.pop(\"NGROK_AUTH_TOKEN\", None)     # comment out if you prefer env-var method\n",
        "conf.PyngrokConfig.config_path = None        # ignore ~/.ngrok2/ngrok.yml\n",
        "\n",
        "# 1ï¸âƒ£ pass the token directly\n",
        "ngrok.set_auth_token(REAL_TOKEN)\n",
        "\n",
        "# 2ï¸âƒ£ confirm what token pyngrok will send\n",
        "print(\"pyngrok will send token:\", conf.get_default().auth_token[:8] + \"â€¦\")\n",
        "assert conf.get_default().auth_token == REAL_TOKEN, \"Token mismatch!\"\n",
        "\n",
        "# 3ï¸âƒ£ quick sanity: what ngrok binary is in PATH?\n",
        "print(\"ngrok version:\", subprocess.check_output([\"ngrok\", \"version\"]).decode().strip())\n",
        "\n",
        "# 4ï¸âƒ£ open a dummy tunnel on an unused port (e.g., 9999) just to test auth\n",
        "try:\n",
        "    test_url = ngrok.connect(9999).public_url\n",
        "    print(\"âœ… auth OK, test tunnel:\", test_url)\n",
        "    ngrok.disconnect(test_url)   # close it\n",
        "except process.PyngrokNgrokError as e:\n",
        "    print(\"âŒ still failing ->\", e)\n",
        "    sys.exit(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIoI6Z_HbRfP"
      },
      "outputs": [],
      "source": [
        "def test_restaurant_detection():\n",
        "    if not restaurant_vectorstores:\n",
        "        print(\"âŒ No restaurant data available for testing. Please upload menu_faq.json first.\")\n",
        "        return\n",
        "\n",
        "    test_queries = [\n",
        "    \"How much protein is in Grilled Salmon?\",\n",
        "    \"What's the protein content in a Cheeseburger?\",\n",
        "    \"How many calories are in a Big Mac?\",\n",
        "    \"How much fat is in Spaghetti Bolognese?\",\n",
        "    \"What is the price of Grilled Salmon?\",\n",
        "    \"What ingredients are in a Grilled Salmon?\",\n",
        "    \"What are the ingredients in Spaghetti Bolognese?\",\n",
        "    \"How many calories are in the Margherita Pizza?\",\n",
        "    \"What is the price of Lasagna?\",\n",
        "    \"Is the Fettuccine Alfredo vegetarian?\",\n",
        "    \"Does Penne Arrabbiata contain gluten?\",\n",
        "    \"What allergens are in Risotto?\",\n",
        "    \"How many times has Caprese Salad been ordered?\",\n",
        "    \"Is Bruschetta vegan?\",\n",
        "    \"What are the allergens in Ravioli?\",\n",
        "    \"How many calories are in the Cheeseburger?\",\n",
        "    \"What is the price of the Grilled Chicken Sandwich?\",\n",
        "    \"Does Hot Dog have any dietary labels?\",\n",
        "    \"What allergens are in BBQ Ribs?\",\n",
        "    \"How many calories are in Grilled Salmon?\",\n",
        "    \"Is Caesar Salad vegetarian?\",\n",
        "    \"How much does Chicken Tenders cost?\",\n",
        "    \"What allergens are in Mac and Cheese?\",\n",
        "    \"What dietary restriction does Buffalo Wings support?\",\n",
        "    \"How many calories are in Big Mac?\",\n",
        "    \"What is the price of Spicy McChicken?\",\n",
        "    \"Does McChicken have a Halal label?\",\n",
        "    \"What ingredients are in Filet-O-Fish?\",\n",
        "    \"Are Chicken McNuggets Halal?\",\n",
        "    \"What is the calorie count of French Fries?\",\n",
        "    \"What dietary label does Hash Browns have?\",\n",
        "    \"How much is a Chocolate Shake?\",\n",
        "    \"What allergens are in McFlurry?\",\n",
        "    \"What are the ingredients of Egg McMuffin?\",\n",
        "    \"What is the price of Sausage Biscuit?\",\n",
        "    \"Is Apple Pie vegan?\",\n",
        "    \"Which dish has the most calories at Italian Bistro?\",\n",
        "    \"What is the average rating of American Grill?\",\n",
        "    \"How many dishes at McDonald's are marked Halal?\",\n",
        "    \"What allergens are in Grilled Chicken Sandwich?\",\n",
        "    \"What is the calorie count of the Margherita Pizza?\",\n",
        "    \"how much calories is in cheeseburger?\",\n",
        "    \"What is the dietary category for Ravioli?\",\n",
        "    \"Which dish contains buttermilk and honey mustard?\",\n",
        "    \"how much for a Big Mac?\",\n",
        "    \"Which restaurant offers the cheapest meal?\"\n",
        "    ]\n",
        "\n",
        "    for query in test_queries:\n",
        "        print(f\"ğŸ§‘ User: {query}\")\n",
        "        response = ask_foodbot(query)\n",
        "        print(f\"ğŸ¤– FoodBot: {response.strip()}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "# Run the test\n",
        "# test_restaurant_detection()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd5WHLHSYVse"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "#  Chat loop that logs every turn to Firebase and re-uses history\n",
        "# -------------------------------------------------------------\n",
        "def chat_with_foodbot(resume_chat_id: str | None = None):\n",
        "    \"\"\"\n",
        "    Start an interactive session with FoodBot.\n",
        "    â€¢ If `resume_chat_id` is None, a brand-new chat node is created.\n",
        "    â€¢ Otherwise we keep adding to the existing chat.\n",
        "    \"\"\"\n",
        "    if not restaurant_vectorstores:\n",
        "        print(\"âŒ No restaurant data available. Upload menu_faq.json and run setup cells first.\")\n",
        "        return\n",
        "\n",
        "    # 1ï¸âƒ£  Pick chat id (new or existing)\n",
        "    if resume_chat_id:\n",
        "        chat_id = resume_chat_id\n",
        "        print(f\"ğŸ”„ Resuming chat {chat_id[:8]}â€¦\")\n",
        "    else:\n",
        "        chat_id = start_new_chat(TEST_USER_ID)   # helper we defined earlier\n",
        "        print(f\"ğŸ’¬ New chat started (id={chat_id[:8]}â€¦). Type 'exit' to quit.\")\n",
        "\n",
        "    print(\"ğŸª Restaurants loaded:\", \", \".join(restaurant_vectorstores.keys()))\n",
        "    print()\n",
        "\n",
        "    # 2ï¸âƒ£  Main loop\n",
        "    while True:\n",
        "        query = input(\"ğŸ§‘ You: \").strip()\n",
        "        if query.lower() in {\"exit\", \"quit\"}:\n",
        "            print(\"ğŸ‘‹ Goodbye!\")\n",
        "            break\n",
        "\n",
        "        print(\"-\" * 60)\n",
        "        print(f\"ğŸ§‘ User: {query}\")\n",
        "\n",
        "        try:\n",
        "            # ask_foodbot now expects (query, chat_id, ...)\n",
        "            response = ask_foodbot(query, chat_id)\n",
        "            print(f\"ğŸ¤– FoodBot: {response.strip()}\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error: {e}\")\n",
        "\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "# ğŸ‘‰ Run this to start a **fresh** chat each session\n",
        "chat_with_foodbot()\n",
        "\n",
        "# ğŸ‘‰ Or pass a previous chat_id string to continue a thread\n",
        "# chat_with_foodbot(resume_chat_id=\"-N9abc123xyz\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci342pDKmz7L"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
